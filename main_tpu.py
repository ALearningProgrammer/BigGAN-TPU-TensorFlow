from BigGAN_512 import BigGAN_512
from BigGAN_256 import BigGAN_256
from BigGAN_128 import BigGAN_128
import argparse
import subprocess
import os.path

from utils import *

"""parsing and configuration"""
def parse_args():
	desc = "Tensorflow implementation of BigGAN"
	parser = argparse.ArgumentParser(description=desc)
	parser.add_argument('--tag'              , action="append" , default=[])
	parser.add_argument('--phase'            , type=str        , default='train'                                           , help='train or test ?')
	
	parser.add_argument('--train-input-path' , type=str        , default='./datasets/atk-vclose/atk-vclose-r07.tfrecords')

	parser.add_argument('--model-dir'        , type=str        , default='model')
	parser.add_argument('--checkpoint-dir'   , type=str        , default='checkpoint')
	parser.add_argument('--result-dir'       , type=str        , default='results')
	parser.add_argument('--log-dir'          , type=str        , default='logs')
	parser.add_argument('--sample-dir'       , type=str        , default='samples')

	# SAGAN
	# batch_size = 256
	# base channel = 64
	# epoch = 100 (1M iterations)

	parser.add_argument('--img-size'        , type=int             , default=128                               , help='The width and height of the input/output image')
	parser.add_argument('--img-ch'          , type=int             , default=3                                 , help='The number of channels in the input/output image')

	parser.add_argument('--train-max-steps' , type=int             , default=10000                             , help='The number of training iterations')
	parser.add_argument('--batch-size'      , dest="_batch_size"   , type=int                                  , default=2048                                             , help='The size of batch across all GPUs')
	parser.add_argument('--ch'              , type=int             , default=96                                , help='base channel number per layer')

	parser.add_argument('--use-tpu'         , action='store_true')
	parser.add_argument('--tpu-name'        , type=str             , default=None)
	parser.add_argument('--num-shards'      , type=int             , default=8) # A single TPU has 8 shards
	parser.add_argument('--steps-per-loop'  , type=int             , default=1000) # A single TPU has 8 shards

	parser.add_argument('--g-lr'            , type=float           , default=0.00005                           , help='learning rate for generator')
	parser.add_argument('--d-lr'            , type=float           , default=0.0002                            , help='learning rate for discriminator')

	# if lower batch size
	# g_lr = 0.0001
	# d_lr = 0.0004

	# if larger batch size
	# g_lr = 0.00005
	# d_lr = 0.0002

	parser.add_argument('--beta1'          , type=float    , default=0.0           , help='beta1 for Adam optimizer')
	parser.add_argument('--beta2'          , type=float    , default=0.9           , help='beta2 for Adam optimizer')
	parser.add_argument('--moving-decay'   , type=float    , default=0.9999        , help='moving average decay for generator')

	parser.add_argument('--z-dim'          , type=int      , default=128           , help='Dimension of noise vector')
	parser.add_argument('--sn'             , type=str2bool , default=True          , help='using spectral norm')

	parser.add_argument('--gan-type'       , type=str      , default='hinge'       , help='[gan / lsgan / wgan-gp / wgan-lp / dragan / hinge]')
	parser.add_argument('--ld'             , type=float    , default=10.0          , help='The gradient penalty lambda')
	parser.add_argument('--n-critic'       , type=int      , default=2             , help='The number of critic')

	parser.add_argument('--sample-num'     , type=int      , default=64            , help='The number of sample images')
	parser.add_argument('--test-num'       , type=int      , default=10            , help='The number of images generated by the test')

	args = parser.parse_args()
	return check_args(args)


def check_args(args):
	check_folder(args.checkpoint_dir)
	check_folder(args.result_dir)
	check_folder(args.log_dir)
	check_folder(args.sample_dir)

	# --epoch
	try:
		assert args.epoch >= 1
	except:
		print('number of epochs must be larger than or equal to one')

	# --batch_size
	try:
		assert args._batch_size >= 1
	except:
		print('batch size must be larger than or equal to one')
	return args


def print_args(args):
	print()

	print("##### Information #####")
	print("# BigGAN 128")
	print("# gan type : ", args.gan_type)
	print("# dataset : ", args.train_input_path)
	print("# batch_size : ", args._batch_size)
	print("# max training steps : ", args.train_max_steps)

	print()

	print("##### Generator #####")
	print("# spectral normalization : ", args.sn)
	print("# learning rate : ", args.g_lr)

	print()

	print("##### Discriminator #####")
	print("# the number of critic : ", args.n_critic)
	print("# spectral normalization : ", args.sn)
	print("# learning rate : ", args.d_lr)


def model_dir(args):
	if args.sn :
		sn = '_sn'
	else :
		sn = ''

	run_name = "{}_{}_{}{}".format(
		 args.gan_type, args.img_size, args.z_dim, sn)

	return os.path.join(args.model_dir, *args.tag, run_name)



def parse_tfrecord_tf(params, record):
	'''
	Parse the records saved using the NVIDIA ProGAN dataset_tool.py

	Data is stored as CHW uint8 with values ranging 0-255
	Size is stored beside image byte strings
	Data is stored in files with suffix -rN.tfrecords

	N = 0 is the largest size, 128x128 in my personal ATK image build

	'''

	features = tf.parse_single_example(record, features={
		'shape': tf.FixedLenFeature([3], tf.int64),
		'data': tf.FixedLenFeature([], tf.string)})
	data = tf.decode_raw(features['data'], tf.uint8)

	# img = tf.reshape(data, features['shape']) # The way from ProGAN
	img = tf.reshape(data, [params['img_ch'], params['img_size'], params['img_size']])

	img = tf.transpose(img, [2,0,1]) # CHW => HWC
	img = tf.cast(img, tf.float32) / 127.5 - 1

	return img



def generic_input_fn(params, path):
	dataset = tf.data.TFRecordDataset([path])
	dataset = dataset.map(lambda record: parse_tfrecord_tf(params, record))
	dataset = dataset.shuffle(1000).repeat()
	dataset = dataset.batch(params['batch_size'], drop_remainder=True)

	return dataset

def train_input_fn(params):
	return generic_input_fn(params, params['train_input_path'])


def main():
	# parse arguments
	args = parse_args()
	if args is None:
	  exit()

	print_args(args)

	gan = BigGAN_128(args)


	if args.use_tpu:
		my_project_name = subprocess.check_output([
			'gcloud','config','get-value','project'])
		my_zone = subprocess.check_output([
			'gcloud','config','get-value','compute/zone'])
		cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(
				tpu_names=[args.tpu_name],
				zone=my_zone,
				project=my_project)
		master = cluster_resolver.get_master()
	else:
		master = ''

	tpu_run_config = tf.contrib.tpu.RunConfig(
		master=master,
		evaluation_master=master,
		model_dir=model_dir(args),
		session_config=tf.ConfigProto(
			allow_soft_placement=True, log_device_placement=True),
		tpu_config=tf.contrib.tpu.TPUConfig(args.steps_per_loop,
											args.num_shards),
	)

	tpu_estimator = tf.contrib.tpu.TPUEstimator(
		model_fn=lambda features, labels, mode, params: gan.tpu_model_fn(features, labels, mode, params),
		config = tpu_run_config,
		use_tpu=args.use_tpu,
		train_batch_size=args._batch_size,
		eval_batch_size=args._batch_size,
		predict_batch_size=args._batch_size,
		params=vars(args),
	)

	if args.phase == 'train':
		tpu_estimator.train(input_fn=train_input_fn, max_steps=args.train_max_steps)



if __name__ == '__main__':
	main()


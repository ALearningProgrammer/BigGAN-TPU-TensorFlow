from BigGAN_512 import BigGAN_512
from BigGAN_256 import BigGAN_256
from BigGAN_128 import BigGAN_128
import argparse
import subprocess
import os.path

from utils import *

"""parsing and configuration"""
def parse_args():
	desc = "Tensorflow implementation of BigGAN"
	parser = argparse.ArgumentParser(description=desc)
	parser.add_argument('--model-name', type=str,default="BigGAN")
	parser.add_argument('--phase', type=str, default='train', help='train or test ?')
	parser.add_argument('--dataset', type=str, default='celebA-HQ', help='[mnist / cifar10 / custom_dataset]')

	# SAGAN
	# batch_size = 256
	# base channel = 64
	# epoch = 100 (1M iterations)

	parser.add_argument('--img-size', type=int, default=128, help='The size of image')
	parser.add_argument('--epoch', type=int, default=50, help='The number of epochs to run')
	parser.add_argument('--iteration', type=int, default=10000, help='The number of training iterations')
	parser.add_argument('--batch-size', type=int, default=2048, help='The size of batch per gpu')
	parser.add_argument('--ch', type=int, default=96, help='base channel number per layer')

	parser.add_argument('--use-tpu', action='store_true')
	parser.add_argument('--tpu-name', type=str, default=None)
	parser.add_argument('--num-shards', type=int, default=8) # A single TPU has 8 shards

	parser.add_argument('--print-freq', type=int, default=1000, help='The number of image_print_freqy')
	parser.add_argument('--save-freq', type=int, default=1000, help='The number of ckpt_save_freq')

	parser.add_argument('--g-lr', type=float, default=0.00005, help='learning rate for generator')
	parser.add_argument('--d-lr', type=float, default=0.0002, help='learning rate for discriminator')

	# if lower batch size
	# g_lr = 0.0001
	# d_lr = 0.0004

	# if larger batch size
	# g_lr = 0.00005
	# d_lr = 0.0002

	parser.add_argument('--beta1', type=float, default=0.0, help='beta1 for Adam optimizer')
	parser.add_argument('--beta2', type=float, default=0.9, help='beta2 for Adam optimizer')
	parser.add_argument('--moving-decay', type=float, default=0.9999, help='moving average decay for generator')

	parser.add_argument('--z-dim', type=int, default=128, help='Dimension of noise vector')
	parser.add_argument('--sn', type=str2bool, default=True, help='using spectral norm')

	parser.add_argument('--gan-type', type=str, default='hinge', help='[gan / lsgan / wgan-gp / wgan-lp / dragan / hinge]')
	parser.add_argument('--ld', type=float, default=10.0, help='The gradient penalty lambda')

	parser.add_argument('--n-critic', type=int, default=2, help='The number of critic')


	parser.add_argument('--sample-num', type=int, default=64, help='The number of sample images')
	parser.add_argument('--test-num', type=int, default=10, help='The number of images generated by the test')

	parser.add_argument('--model-dir', type=str, default='./model')
	parser.add_argument('--train-input-dir', type=str, default='./datasets/train.tfrecord')

	parser.add_argument('--checkpoint-dir', type=str, default='checkpoint')
	parser.add_argument('--result-dir', type=str, default='results')
	parser.add_argument('--log-dir', type=str, default='logs')
	parser.add_argument('--sample-dir', type=str, default='samples')

	return check_args(parser.parse_args())


def check_args(args):
	check_folder(args.checkpoint_dir)
	check_folder(args.result_dir)
	check_folder(args.log_dir)
	check_folder(args.sample_dir)

	# --epoch
	try:
		assert args.epoch >= 1
	except:
		print('number of epochs must be larger than or equal to one')

	# --batch_size
	try:
		assert args.batch_size >= 1
	except:
		print('batch size must be larger than or equal to one')
	return args



def model_dir(args):
	if args.sn :
		sn = '_sn'
	else :
		sn = ''

	run_name = "{}_{}_{}_{}_{}{}".format(
		args.model_name, args.dataset_name, args.gan_type, args.img_size, args.z_dim, sn)

	return os.path.join(args.model_dir, run_name)



def parse_tfrecord_tf(record):
	'''
	Parse the records saved using the NVIDIA ProGAN dataset_tool.py

	Data is stored as CHW uint8 with values ranging 0-255
	Size is stored beside image byte strings
	Data is stored in files with suffix -rN.tfrecords

	N = 0 is the largest size, 128x128 in my personal ATK image build

	'''

	features = tf.parse_single_example(record, features={
		'shape': tf.FixedLenFeature([3], tf.int64),
		'data': tf.FixedLenFeature([], tf.string)})
	data = tf.decode_raw(features['data'], tf.uint8)
	img = tf.reshape(data, features['shape'])
	img = tf.transpose(img, [2,0,1]) # CHW => HWC
	img = tf.cast(img, tf.float32) / 127.5 - 1

	return img



def generic_input_fn(path):
	dataset = tf.data.TFRecordDataset([path])
	dataset = dataset.map(parse_tfrecord_tf)
	dataset = dataset.shuffle(1000).repeat()
	dataset = dataset.batch(params.batch_size, drop_remainder=True)

	return dataset

def train_input_fn(params):
	return generic_input_fn(params.train_input_path)

  
def eval_input_fn(params):
	# TODO: make this contain N zeros
	return generic_input_fn(params.train_input_path)


"""main"""
def main():
	# parse arguments
	args = parse_args()
	if args is None:
	  exit()


	gan = BigGAN_128(args)


	if args.use_tpu:
		my_project_name = subprocess.check_output([
			'gcloud','config','get-value','project'])
		my_zone = subprocess.check_output([
			'gcloud','config','get-value','compute/zone'])
		cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(
				tpu_names=[args.tpu_name],
				zone=my_zone,
				project=my_project)
		master = cluster_resolver.get_master()
	else:
		master = ''

	tpu_run_config = tf.contrib.tpu.RunConfig(
		master=master,
		evaluation_master=master,
		model_dir=model_dir,
		session_config=tf.ConfigProto(
			allow_soft_placement=True, log_device_placement=True),
		tpu_config=tf.contrib.tpu.TPUConfig(args.iteration,
											args.num_shards),
	)

	tpu_estimator = tf.contrib.tpu.TPUEstimator(
		model_fn=lambda features, labels, mode, params: gan.tpu_model_fn(features, labels, mode, params),
		config = tpu_run_config,
		use_tpu=args.use_tpu,
		train_batch_size=args.batch_size,
		eval_batch_size=args.batch_size,
		predict_batch_size=args.batch_size,
		params=args,
	)

	if args.phase == 'train':
		# launch the graph in a session
		tpu_estimator.train(input_fn=train_input_fn, max_steps=args.train_steps)



if __name__ == '__main__':
	main()
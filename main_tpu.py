from BigGAN_512 import BigGAN_512
from BigGAN_256 import BigGAN_256
from BigGAN_128 import BigGAN_128
import argparse
import subprocess
import os.path

from utils import *

"""parsing and configuration"""
def parse_args():
	desc = "Tensorflow implementation of BigGAN"
	parser = argparse.ArgumentParser(description=desc)
	parser.add_argument('--tag'              , action="append" , default=[])
	parser.add_argument('--phase'            , type=str        , default='train'                                           , help='train or test ?')
	
	parser.add_argument('--train-input-path' , type=str        , default='./datasets/atk-vclose/atk-vclose-r07.tfrecords')
	parser.add_argument('--model-dir'        , type=str        , default='model')
	parser.add_argument('--result-dir'       , type=str        , default='results')
	parser.add_argument('--log-dir'          , type=str        , default='logs')
	parser.add_argument('--sample-dir'       , type=str        , default='samples')

	# SAGAN
	# batch_size = 256
	# base channel = 64
	# epoch = 100 (1M iterations)

	parser.add_argument('--img-size'        , type=int             , default=128                               , help='The width and height of the input/output image')
	parser.add_argument('--img-ch'          , type=int             , default=3                                 , help='The number of channels in the input/output image')

	parser.add_argument('--epochs'          , type=int             , default=100                               , help='The number of training iterations')
	parser.add_argument('--train-steps'     , type=int             , default=10000                             , help='The number of training iterations')
	parser.add_argument('--eval-steps'      , type=int             , default=100                               , help='The number of eval iterations')
	parser.add_argument('--batch-size'      , type=int             , default=2048  , dest="_batch_size"        , help='The size of batch across all GPUs')
	parser.add_argument('--ch'              , type=int             , default=96                                , help='base channel number per layer')

	parser.add_argument('--use-tpu'         , action='store_true')
	parser.add_argument('--tpu-name'        , action='append'      , default=[] , type=str             )
	parser.add_argument('--tpu-zone'		, type=str, default='us-central1-f')
	parser.add_argument('--num-shards'      , type=int             , default=8) # A single TPU has 8 shards
	parser.add_argument('--steps-per-loop'  , type=int             , default=10000)

	parser.add_argument('--g-lr'            , type=float           , default=0.00005                           , help='learning rate for generator')
	parser.add_argument('--d-lr'            , type=float           , default=0.0002                            , help='learning rate for discriminator')

	# if lower batch size
	# g_lr = 0.0001
	# d_lr = 0.0004

	# if larger batch size
	# g_lr = 0.00005
	# d_lr = 0.0002

	parser.add_argument('--beta1'          , type=float    , default=0.0           , help='beta1 for Adam optimizer')
	parser.add_argument('--beta2'          , type=float    , default=0.9           , help='beta2 for Adam optimizer')
	parser.add_argument('--moving-decay'   , type=float    , default=0.9999        , help='moving average decay for generator')

	parser.add_argument('--z-dim'          , type=int      , default=128           , help='Dimension of noise vector')
	parser.add_argument('--sn'             , type=str2bool , default=True          , help='using spectral norm')

	parser.add_argument('--gan-type'       , type=str      , default='hinge'       , help='[gan / lsgan / wgan-gp / wgan-lp / dragan / hinge]')
	parser.add_argument('--ld'             , type=float    , default=10.0          , help='The gradient penalty lambda')
	parser.add_argument('--n-critic'       , type=int      , default=2             , help='The number of critic')

	parser.add_argument('--sample-num'     , type=int      , default=36            , help='The number of images to generate')
	parser.add_argument('--test-num'       , type=int      , default=10            , help='The number of images generated by the test')

	parser.add_argument('--verbosity', type=str, default='WARNING')

	args = parser.parse_args()
	return check_args(args)


def check_args(args):
	check_folder(args.result_dir)
	check_folder(os.path.join(args.result_dir, model_name(args)))
	check_folder(args.log_dir)
	check_folder(args.sample_dir)
	check_folder(os.path.join(args.sample_dir, model_name(args)))

	assert args.epochs >= 1, "number of epochs must be larger than or equal to one"
	assert args._batch_size >= 1, "batch size must be larger than or equal to one"
	assert args.ch >= 8, "--ch cannot be less than 8 otherwise some dimensions of the network will be size 0"

	return args


def print_args(args):
	print()

	print("##### Information #####")
	print("# BigGAN 128")
	print("# gan type : ", args.gan_type)
	print("# dataset : ", args.train_input_path)
	print("# batch_size : ", args._batch_size)
	print("# training steps : ", args.train_steps)
	print("# epochs : ", args.epochs)


	print()

	print("##### Generator #####")
	print("# spectral normalization : ", args.sn)
	print("# learning rate : ", args.g_lr)

	print()

	print("##### Discriminator #####")
	print("# the number of critic : ", args.n_critic)
	print("# spectral normalization : ", args.sn)
	print("# learning rate : ", args.d_lr)

	print()


def model_name(args):
	if args.sn :
		sn = '_sn'
	else :
		sn = ''

	return "{}_{}_{}{}_{}".format(
		 args.gan_type, args.img_size, args.z_dim, sn, args.ch)

def model_dir(args):
	return os.path.join(args.model_dir, *args.tag, model_name(args))



def parse_tfrecord_tf(params, record):
	'''
	Parse the records saved using the NVIDIA ProGAN dataset_tool.py

	Data is stored as CHW uint8 with values ranging 0-255
	Size is stored beside image byte strings
	Data is stored in files with suffix -rN.tfrecords

	N = 0 is the largest size, 128x128 in my personal ATK image build

	'''

	features = tf.parse_single_example(record, features={
		'shape': tf.FixedLenFeature([3], tf.int64),
		'data': tf.FixedLenFeature([], tf.string)})
	data = tf.decode_raw(features['data'], tf.uint8)

	# img = tf.reshape(data, features['shape']) # The way from ProGAN
	img = tf.reshape(data, [params['img_ch'], params['img_size'], params['img_size']])

	img = tf.transpose(img, [1,2,0]) # CHW => HWC
	img = tf.cast(img, tf.float32) / 127.5 - 1

	return img



def generic_input_fn(params, path, repeat=False):
	dataset = tf.data.TFRecordDataset([path])
	dataset = dataset.map(lambda record: parse_tfrecord_tf(params, record))
	dataset = dataset.shuffle(1000)

	if repeat:
		dataset = dataset.repeat()

	dataset = dataset.batch(params['batch_size'], drop_remainder=True)

	return dataset

def train_input_fn(params):
	return generic_input_fn(params, params['train_input_path'], repeat=True)

def eval_input_fn(params):
	return generic_input_fn(params, params['train_input_path'])

def predict_input_fn(params):
	data = np.zeros([params['sample_num'], 1])
	dataset = tf.data.Dataset.from_tensor_slices(data)
	dataset = dataset.batch(params['batch_size'], drop_remainder=True)
	return dataset


def main():
	# parse arguments
	args = parse_args()
	if args is None:
	  exit()

	print_args(args)

	gan = BigGAN_128(args)

	if args.use_tpu:
		# my_project_name = subprocess.check_output([
		# 	'gcloud','config','get-value','project'])
		# my_zone = subprocess.check_output([
		# 	'gcloud','config','get-value','compute/zone'])

		cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(
				tpu=args.tpu_name,
				zone=args.tpu_zone,
				# project=my_project_name
				)
		master = cluster_resolver.get_master()
	else:
		master = ''

	tpu_run_config = tf.contrib.tpu.RunConfig(
		master=master,
		evaluation_master=master,
		model_dir=model_dir(args),
		session_config=tf.ConfigProto(
			allow_soft_placement=True, 
			log_device_placement=False),
		tpu_config=tf.contrib.tpu.TPUConfig(args.steps_per_loop,
											args.num_shards),
	)

	tpu_estimator = tf.contrib.tpu.TPUEstimator(
		model_fn=lambda features, labels, mode, params: gan.tpu_model_fn(features, labels, mode, params),
		config = tpu_run_config,
		use_tpu=args.use_tpu,
		train_batch_size=args._batch_size,
		eval_batch_size=args._batch_size,
		predict_batch_size=args._batch_size,
		params=vars(args),
	)

	tf.logging.set_verbosity(args.verbosity)

	if args.phase == 'train':
		for epoch in range(args.epochs):
			print(f"Training epoch {epoch}")
			tpu_estimator.train(input_fn=train_input_fn, steps=args.train_steps)
			
			print(f"Evaluate {epoch}")
			evaluation = tpu_estimator.evaluate(input_fn=eval_input_fn, steps=args.eval_steps)
			print(evaluation)
			save_evaluation(args, evaluation, epoch, model_name(args))

			print(f"Generate predictions {epoch}")
			predictions = tpu_estimator.predict(input_fn=predict_input_fn)
			
			print(f"Save predictions")
			save_predictions(args, predictions, epoch, model_name(args))




if __name__ == '__main__':
	main()

